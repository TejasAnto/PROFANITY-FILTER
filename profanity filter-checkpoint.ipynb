{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5118bb-71ef-488d-891b-71e56623a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from transformers import pipeline\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab34230-bc74-4f4c-aa80-676a36547702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "\n",
    "#PART 1: INITIALIZE MODULES\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "#Bad Word Censoring\n",
    "BAD_WORDS = [\"shit\", \"idiot\", \"die\", \"scum\", \"moron\", \"retard\"]\n",
    "\n",
    "def censor_text(text):\n",
    "    for word in BAD_WORDS:\n",
    "        text = re.sub(rf\"\\b{word}\\b\", \"*\" * len(word), text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "#Sentiment Analysis (VADER)\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    score = sia.polarity_scores(text)[\"compound\"]\n",
    "    return \"Positive\" if score > -0.3 else \"Negative\"\n",
    "\n",
    "#Toxicity Detection (RoBERTa)\n",
    "toxicity_model = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"facebook/roberta-hate-speech-dynabench-r4-target\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "def check_toxicity(text):\n",
    "    result = toxicity_model(text)[0]\n",
    "    return {\"score\": result[\"score\"] if result[\"label\"] == \"hate\" else 0.0}\n",
    "\n",
    "#PART 2: CORE FILTERING LOGIC\n",
    "def classify_intent(text):\n",
    "    toxicity = check_toxicity(text)\n",
    "    sentiment = analyze_sentiment(text)\n",
    "    \n",
    "    if toxicity[\"score\"] > 0.85 or (toxicity[\"score\"] > 0.5 and sentiment == \"Negative\"):\n",
    "        return \"aggressive\"\n",
    "    return \"funny\"\n",
    "\n",
    "def process_input(text):\n",
    "    intent = classify_intent(text)\n",
    "    return (False, \"[BLOCKED]\") if intent == \"aggressive\" else (True, censor_text(text))\n",
    "\n",
    "#PART 3: INTERACTIVE SYSTEM\n",
    "def main():\n",
    "    print(\"\"\"\n",
    "    <<< SMART PROFANITY FILTER >>>\n",
    "    Type any message and press Enter.\n",
    "    Messages will be censored or blocked based on content.\n",
    "    Type 'quit', 'exit', or 'q' to end the program.\n",
    "    \"\"\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYour message: \").strip()\n",
    "\n",
    "        if user_input.lower() in ('quit', 'exit', 'q'):\n",
    "            print(\"\\nGoodbye! Filter session ended.\")\n",
    "            break\n",
    "            \n",
    "        is_allowed, output = process_input(user_input)\n",
    "        \n",
    "        if not is_allowed:\n",
    "            print(\"\\nBLOCKED: This message contains offensive content.\")\n",
    "            print(\"Reason: Detected as aggressive/hateful\")\n",
    "            print(\"Please rephrase your message.\")\n",
    "        else:\n",
    "            print(\"\\nFILTERED MESSAGE:\", output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f92f6-1620-4a35-bb2a-c512a72dde6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
